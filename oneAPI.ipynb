{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "MqMot_ZAmyMR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqMot_ZAmyMR",
    "outputId": "067310ae-1341-4b4c-84d6-53212ce7964c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.9/site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: tqdm in /glob/development-tools/versions/oneapi/2023.0.1/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in /glob/development-tools/versions/oneapi/2023.0.1/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.9/site-packages (from nltk) (8.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/u190532/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/u190532/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/u190532/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eUWkp5OJmd2L",
   "metadata": {
    "id": "eUWkp5OJmd2L"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"./training_data.csv\")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "df = df.drop(['Source', 'PublishDate'], axis=1)\n",
    "\n",
    "# Remove missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Define stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens to form text again\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Preprocess text in the dataset\n",
    "df['clean_title'] = df['Title'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "LTnRET8Cml6A",
   "metadata": {
    "id": "LTnRET8Cml6A"
   },
   "outputs": [],
   "source": [
    "df['clean_headline'] = df['Headline'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Hy1tLJsamw2j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hy1tLJsamw2j",
    "outputId": "c88e52a8-7dd0-4180-b168-28be18cd3ee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /glob/development-tools/versions/oneapi/2023.0.1/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./.local/lib/python3.9/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in ./.local/lib/python3.9/site-packages (from gensim) (1.22.4)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.1\n",
      "(0, '0.059*\"economy\" + 0.023*\"china\" + 0.014*\"new\" + 0.011*\"budget\" + 0.008*\"’\" + 0.008*\"show\" + 0.007*\"sander\" + 0.006*\"pick\" + 0.006*\"market\" + 0.006*\"economic\"')\n",
      "(1, '0.101*\"microsoft\" + 0.016*\"window\" + 0.016*\"microsofts\" + 0.014*\"10\" + 0.012*\"new\" + 0.011*\"surface\" + 0.008*\"one\" + 0.007*\"make\" + 0.007*\"cloud\" + 0.007*\"lumia\"')\n",
      "(2, '0.130*\"obama\" + 0.030*\"obamas\" + 0.023*\"palestine\" + 0.020*\"president\" + 0.013*\"trump\" + 0.012*\"palestinian\" + 0.010*\"court\" + 0.009*\"state\" + 0.009*\"say\" + 0.007*\"visit\"')\n",
      "(3, '0.097*\"economy\" + 0.016*\"u\" + 0.012*\"2016\" + 0.012*\"economic\" + 0.011*\"say\" + 0.011*\"global\" + 0.010*\"growth\" + 0.008*\"year\" + 0.007*\"oil\" + 0.006*\"gun\"')\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "import gensim\n",
    "\n",
    "# Convert preprocessed text to gensim dictionary and corpus\n",
    "texts = [doc.split() for doc in df['clean_title']]\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Define number of topics and run LDA model\n",
    "num_topics = 4\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print top words in each topic\n",
    "for topic in lda_model.show_topics(num_topics=num_topics):\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7KHgzObsnm92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7KHgzObsnm92",
    "outputId": "01c06198-002a-4dc1-a19e-8d5f357be4e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.015*\"microsoft\" + 0.010*\"new\" + 0.009*\"company\" + 0.008*\"business\" + 0.007*\"cloud\" + 0.006*\"palestine\" + 0.006*\"service\" + 0.005*\"research\" + 0.004*\"technology\" + 0.004*\"help\"')\n",
      "(1, '0.053*\"microsoft\" + 0.014*\"new\" + 0.014*\"window\" + 0.010*\"10\" + 0.008*\"microsofts\" + 0.008*\"surface\" + 0.008*\"’\" + 0.007*\"company\" + 0.007*\"one\" + 0.005*\"user\"')\n",
      "(2, '0.052*\"economy\" + 0.023*\"economic\" + 0.011*\"global\" + 0.011*\"said\" + 0.010*\"market\" + 0.010*\"china\" + 0.009*\"u\" + 0.009*\"minister\" + 0.008*\"world\" + 0.007*\"government\"')\n",
      "(3, '0.027*\"economy\" + 0.022*\"year\" + 0.014*\"percent\" + 0.012*\"2015\" + 0.010*\"quarter\" + 0.010*\"last\" + 0.009*\"month\" + 0.008*\"u\" + 0.008*\"growth\" + 0.006*\"palestine\"')\n",
      "(4, '0.063*\"obama\" + 0.057*\"president\" + 0.033*\"barack\" + 0.010*\"state\" + 0.010*\"said\" + 0.010*\"obamas\" + 0.010*\"u\" + 0.009*\"palestinian\" + 0.009*\"republican\" + 0.008*\"house\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# Convert preprocessed text to gensim dictionary and corpus\n",
    "texts = [doc.split() for doc in df['clean_headline']]\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Define number of topics and run LDA model\n",
    "num_topics = 5\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print top words in each topic\n",
    "for topic in lda_model.show_topics(num_topics=num_topics):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6Xqpg1VKowL9",
   "metadata": {
    "id": "6Xqpg1VKowL9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32406, 18481)\n",
      "(8102, 18481)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_title'], df['SentimentTitle'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train_vec.shape)\n",
    "print(X_test_vec.shape)\n",
    "x_title = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "kgbrUMgeqXbK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgbrUMgeqXbK",
    "outputId": "ef59f2f0-f5d5-495e-ffb6-7346211c517a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.000000    6645\n",
      " 0.041667     409\n",
      " 0.044194     368\n",
      "-0.041667     299\n",
      " 0.039528     283\n",
      "             ... \n",
      "-0.036182       1\n",
      " 0.027862       1\n",
      "-0.091213       1\n",
      " 0.073546       1\n",
      "-0.173529       1\n",
      "Name: SentimentTitle, Length: 6601, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7v5RMIn1pMhv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7v5RMIn1pMhv",
    "outputId": "2fbbe449-873f-4ab0-bd0a-c0938a51df73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/inteloneapi/intelpython/latest/lib/python3.9/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train SVM classifier\n",
    "svmTitle = LinearSVR()\n",
    "\n",
    "svmTitle.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = svmTitle.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ivTeEco5qrDa",
   "metadata": {
    "id": "ivTeEco5qrDa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32406, 33111)\n",
      "(8102, 33111)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_headline'], df['SentimentHeadline'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "print(X_train_vec.shape)\n",
    "print(X_test_vec.shape)\n",
    "x_Headline = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "k4BQQM_zq-jV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4BQQM_zq-jV",
    "outputId": "d0654d60-e9a6-411e-a85d-727ba67a189e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/inteloneapi/intelpython/latest/lib/python3.9/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train SVM classifier\n",
    "svmHeadline = LinearSVR()\n",
    "svmHeadline.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = svmHeadline.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "CVzR_VGWtHeW",
   "metadata": {
    "id": "CVzR_VGWtHeW"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dfTest = pd.read_csv(\"./test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "qymOoP9Qy_jq",
   "metadata": {
    "id": "qymOoP9Qy_jq"
   },
   "outputs": [],
   "source": [
    "dfTest['clean_title'] = dfTest['Title'].apply(preprocess_text)\n",
    "dfTest['clean_headline'] = dfTest['Headline'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d--H9WG-yVbX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d--H9WG-yVbX",
    "outputId": "296ec4f7-c7b0-4421-be40-4dc7103802c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32406, 18481)\n",
      "(15424, 18481)\n",
      "[-0.00225646  0.07821524 -0.019051   ...  0.00142398  0.09807077\n",
      "  0.05414269]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(x_title)\n",
    "X_test_vec = vectorizer.transform(dfTest['clean_title'])\n",
    "print(X_train_vec.shape)\n",
    "print(X_test_vec.shape)\n",
    "y_predTitle = svmTitle.predict(X_test_vec)\n",
    "print(y_predTitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "wYKaIjn8yzvJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYKaIjn8yzvJ",
    "outputId": "a742449a-1e3d-4e2a-f66e-555ed2a8db8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00776357  0.1488848  -0.23006005 ... -0.02424033 -0.05251094\n",
      "  0.17786623]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(x_Headline)\n",
    "X_test_vec = vectorizer.transform(dfTest['clean_headline'])\n",
    "\n",
    "y_predHeadline = svmHeadline.predict(X_test_vec)\n",
    "print(y_predHeadline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "Gbg_hQZkzSu0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gbg_hQZkzSu0",
    "outputId": "1c23a476-5593-4819-9cee-2ddbf4ca74fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_predTitle))\n",
    "print(type(y_predHeadline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "LUmfb1t3zfWt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LUmfb1t3zfWt",
    "outputId": "5b58f1ea-089d-4529-8a1d-3086123b7436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "y_idLink = dfTest['IDLink'].to_numpy()\n",
    "print(type(y_idLink))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2khrTXxBzsqX",
   "metadata": {
    "id": "2khrTXxBzsqX"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"IDLink\": y_idLink,\n",
    "    \"SentimentTitle\": y_predTitle,\n",
    "    \"SentimentHeadline\": y_predHeadline\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17d370-c4d9-400d-a6b1-e1c978551ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.0)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
